# Lesson 6
This lesson focused on random forests. I had never heard of this concept before and thought it must have something to do with forests in the natural sense:

![forest](https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41477-019-0374-3/MediaObjects/41477_2019_374_Figa_HTML.jpg)

I was very wrong! The trees were not real trees with trunk and leaves but binary trees. The lesson started with again analysing the titanic data and determining binary splits that could be utilised to predict survival. For example, making a prediction purely based on whether the passenger was male or female gave fairly good results. Perhaps better results than a slow and complex model, which I found pretty cool! Complex does not always equal better! 

Splitting based on one parameter is defined as a OneR classifier but we can go further still! The data split using the first rule can be split again and again and again until an accurate prediction results. This forms the aforementioned binary trees or DecisionTreeClassifier which look like this:

![binary tree](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/63b57041-42a1-44bb-aa1d-9f79a304ed77)

To figure out if a good prediction can be made, the indicator gini is used. For example, from the above binary tree it is a good idea to be a well-off female!

So we have one tree for the forest. To generate the random forest, multiple decision trees can be generated. This is based on the simple but powerful fact that if you can generate a whole bunch of uncorrelated, unbiased models, can average them and get better than any model by itself!

This fastai course has also been my first introduction to the concept of features and feature engineering. I had heard the term before but never knew the meaning. The DecisionTreeClassifier has a cool feature called feature_importances, which plots the features and gives you the most important features in terms of being a predictor.
