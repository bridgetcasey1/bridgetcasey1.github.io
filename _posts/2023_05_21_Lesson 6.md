# Lesson 6
## Random Forests
Lesson 6 focused on **random forests**. I had never heard of this concept before and initially thought it had something to do with forests in the natural sense.

![forest](https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41477-019-0374-3/MediaObjects/41477_2019_374_Figa_HTML.jpg)

I was very wrong! The so-called forest was not referring to a collection of real trees with trunks and leaves but binary trees! The lesson started with analysing the titanic data from previous lessons, but this time determining binary splits that could be utilised to predict survival. For example, making a prediction purely based on whether the passenger was male or female gave fairly good results. Perhaps better results than a slow and complex model, which I found pretty cool! Complex does not always equal better! 

The data can however be split more than once and a binary tree formed using a ```DecisionTreeClassifier```:

![binary tree](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/63b57041-42a1-44bb-aa1d-9f79a304ed77)

To determine if the binary splits give an accurate prediction, the indicator Gini is used. In brief, this value is how likely it is to get the same outcome pulling randomly from the data set, with 0 indicating the exact same outcome for all data points. From the above binary tree, it was a good idea to be a well-off female with lower-class male passengers having a Gini value of 0.06 (most did not survive!). A study I found interesting on random forests and Gini importance can be found [here](/pdf/1471-2105-10-213.pdf) and further explanation of random forests [here](https://towardsdatascience.com/understanding-random-forest-58381e0602d2).

So, at this stage we have generated one tree. The random forest is simply a collection of these trees and the beauty of it is that the average of this collection of uncorrelated, unbiased binary trees will always give a better result than any singular model!

## Side Note
This fastai course has introduced me to the concept of features and feature engineering. I had heard the term before but never knew the meaning. The ```DecisionTreeClassifier``` has a cool function called feature_importances, which plots the features in terms of importance as a predictor.
