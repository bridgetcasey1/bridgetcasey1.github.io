# Lesson 7
## Gradient Accumulation and GPU Memory
The first part of lesson 7 discussed the increased GPU memory required when working with larger models. Larger, more complex models involve more parameters, which allows for more precise adjustment and increased accuracy, however the trade-off is increased GPU memory usage with gradient calculations. Jeremy showed us a handy function for determining how much memory is used for training a model and to then clear out the memory:

```
import gc
def report_gpu():
    print(torch.cuda.list_gpu_processes())
    gc.collect()
    torch.cuda.empty_cache()
```

The above function can be used to compare GPU memory usage for different architectures. More information on GPU can be found [here](https://docs.fast.ai/dev/gpu.html).

The biggest takeaway from the lesson in my opinion was the concept of gradient accumulation. This allows the use of smaller batch sizes and thus less GPU memory to achieve the same performance as a larger batch size (called the effective batch size). It works by accumulating the gradients for multiple batches and then updating model weights accordingly rather than updating the model weights after every batch based on that batch's gradients[^1]. So many clever tricks in deep learning!

## Collaborative Filtering
The deep learning applications I am aware of continues to grow and grow with this lesson covering recommendation algorithms in the context of making movie recommendations based on previous user reviews.

![Movie Time](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/76d22afb-edaa-4079-941a-5c5f2e2d2969)

I have been impressed by recommendations made to me by algorithms in the past, so it was cool to learn how this works! I found the idea that you don't need to identify parameters that influence movie preferences but just indicate that some undefined parameters exist using latent factors and let fastai do the rest!

[^1]: [A useful gradient accumulation resource](https://www.run.ai/blog/gradient-accumulation)
