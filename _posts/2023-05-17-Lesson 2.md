# Lesson 2
This lesson focused on the full deep learning application process in the context of **building a bear classifier**.

![different bear types](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/5bb1662c-f2d9-4451-aabc-598963f4b20b)

## The Process of Building a Bear Classifier
The journey begins with gathering data, which involved downloading images from the internet using the same method as lesson 1. 

![loading bar](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/75088cc2-9d91-4295-9780-ab2c5d96a9f4)

After this, **the model is trained and then the data is cleaned**. Yes, you heard me correctly. I found this part of the lesson surprising as it makes more sense to me to clean the data and then train the model but by the end of the lesson I was convinced that this was the correct way. The model was trained as in lesson 1, with a ```Learner``` created and then fine-tuned. To clean the data, ```ImageClassifierCleaner``` was then utilised with the trained ```Learner``` as the input:

```
cleaner = ImageClassifierCleaner(learn)
cleaner
```
This brings up a GUI as below:

![ImageClassifierCleaner GUI](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/82a7356a-67e3-4b72-9080-cb3ba0c9ab67)

The GUI can be used to delete images that do not fit into any category, such as two bears in one image as shown above. The GUI can also be used to move images to their correct category if they were initially mislabelled. This was something that I had naively never considered but obviously should consider as training a model on incorrect data would significantly impact performance. As the GUI shows results in order of loss, cases of mislabelling can be easily detected as the model will most likely make a "wrong" prediction with high confidence. After completing lesson 2, I enjoyed independently looking more into this topic and if it has interested you as well I found this article worth a read: [Targeting and Removing Bad Training Data](https://towardsdatascience.com/targeting-and-removing-bad-training-data-8ccdac5e7cc3).

## Lesson 2 Extras 
I thought I would also share a few things I found interesting from lesson 2, which included:
- That loss is not just about being right or wrong but also about how confident the model is about the classification, for example high loss can result from the model:
  - Being right with low confidence
  - Being wrong with high confidence
- How easily a trained model can be transferred to another file for application in a new setting![^1]

[^1]: [Saving a basic fastai model](https://www.kaggle.com/code/jhoward/saving-a-basic-fastai-model)
