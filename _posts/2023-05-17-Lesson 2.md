# Lesson 2
This lesson focused on the full deep learning application process in the context of **building a bear classifier**.

![different bear types](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/5bb1662c-f2d9-4451-aabc-598963f4b20b)

The journey begins with gathering data, which involved downloading images from the internet using the same method as lesson 1. 
![loading bar](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/75088cc2-9d91-4295-9780-ab2c5d96a9f4)

After this, **the model is trained and then the data is cleaned**. Yes, you heard me correctly. I found this part of the lesson surprising as it makes more sense to me to clean the data and then train the model but by the end of the lesson I was convinced that this was the way. The model was trained as in lesson 1 and then the ```Learner``` created and fine-tuned used to help clean the data:

```
cleaner = ImageClassifierCleaner(learn)
cleaner
```
This brings up a GUI as below:

![ImageClassifierCleaner GUI](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/82a7356a-67e3-4b72-9080-cb3ba0c9ab67)

The GUI can be used to delete images that do not fit into any category (e.g. two bears in one image) or to move images to their correct category. One thing I haven't thought too much about is the fact that you can train a model on incorrect data. For example, when searching for black bears an image of a grizzly bear may have been incorrectly labelled elsewhere and so accidentally downloaded and stored as a black bear. The data cleaning post training the model can help us easily identify these mistakes. 

A few other things I found interesting from the lesson included:
- Loss is not just about being right or wrong but about how confident the model is about the classification - 
  - Being right with low confidence
  - Being wrong with high confidence
- How easily a trained model can be transferred to another file for application in a new setting.
