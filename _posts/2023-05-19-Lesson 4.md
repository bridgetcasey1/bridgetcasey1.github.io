# Lesson 4
This lesson focused on Natural Language Processing (NLP) and fine tuning a pre-trained NLP model using the Hugging Face Transformers library. And actually didnâ€™t use fastai at all! This was all very new to me but one thing I found noteworthy was the fact that to get good at being a language model, a neural network needs to be good at a lot of things, not just about language and how it is structured but also knowledge about the topic at hand, which could include all sorts of information about the world.

I think it's cool that NLP can be used for sentiment analysis with the example given of determining if a IMDb rating is negative or positive. It reminded me of an application that a friend of mine was working on at a bank, which involved detecting and classifying complaints in real-time.

This was also my first introduction to the Hugging Face hug where pre-trained models can be found with the idea being that if a model was developed to solve a similar problem or trained using similar data then it's a good starting point for the current problem. 

![standing on the shoulders of giants quote](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/5c096fd6-ed7c-4acd-bb06-c33edbba95c4)

This lesson also introduced me to the concept of overfitting and underfitting. Underfitting is where the model does not fit the data well at all and systematically incorrect:

![underfit example](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/be9656f4-1fb7-417d-8af6-ed8227a25619)

Overfitting is where the model fits the exact data points extremely well:
![overfit example](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/574446f2-1878-4b2a-9118-c25ec5405672)

However results in poor performance with the addition of extra data. It is easy to detect underfitting as the training data will not be well represented. Overfitting is trickier! I was then taught the usefulness of the validation set used when training a model. From the original dataset, ~20% is removed and the model is trained to fit the remaining data points and then performance is tested using the removed data points. Simple but brilliant! 
