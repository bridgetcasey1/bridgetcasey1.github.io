# Lesson 7
## Gradient Accumulation and GPU Memory
The first part of lesson 7 discussed the required GPU memory with larger models that require more and more parameters. Although increased accuracy is usually achieved, the gradients that have to be calculated chew up a lot of memory. Jeremy showed us a handy function function to find out how much memory is used for training a model and also to then clear out the memory:

```
import gc
def report_gpu():
    print(torch.cuda.list_gpu_processes())
    gc.collect()
    torch.cuda.empty_cache()
```

The above can be used to compare GPU memory usage for different architectures. The biggest takeaway from the lesson in my opinion was the concept of gradient accumulation. This allows us to use smaller batch sizes and thus less GPU memory but to get results of using a larger (effective) batch size. It works by accumulating the gradients for multiple batches and then updating model weights accordingly rather than updating the model weights after every batch based on that batch's gradients. So many clever tricks in deep learning!

## Multi-Target Models
On the topic of the genius of deep learning, Jeremy also demonstrated how easily the code we have been using can be adapted so that two predictions are made per image. The model has more to deal with now, it can actually improve the model compared to focusing on one category as the extra information may actually help the original prediction.

## Collaborative Filtering
Nearing the end of the course and the applications for deep learning keeps on growing and growing. This lesson also covers recommendation algorithms in the context of making movie recommendations based on previous user movie reviews.

![Movie Time](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/76d22afb-edaa-4079-941a-5c5f2e2d2969)

I have been impressed by recommendations made to me by algorithms in the past so found it cool to learn about how these actually work! I found the idea that you don't need to identify parameters that influence movie preference but just use latent factors to say some exist and let the model do the rest. Any by the rest, I mean optimise the model in terms of loss function and determining parameter values.

# Lesson 8
The last lesson! I have learnt a ridiculous amount during this course and feel it is only scraping the surface of deep learning and AI. I'm glad CNNs (convolutional neural networks) were discussed this lesson as it is a term I always here in deep learning discussions but until this course really had no idea what they referred to. I always thought it was a very abstract concept however through the course have realised a neural network is just a mathematical function. A standard neural network is presented which:
1. Multiplies each input by a number of values (parameters)
2. Sums these for each group of values
3. Replaces the negative numbers with zeros

This process is repeated to increase the number of layers of the neural network, with the input of one layer feeding into the next. So what do we do with this? As the parameter values were chosen randomly the network at first has no real function but these parameters can be adjusted using gradient descent to fit the data at hand. This allows application to so many different problems including, you guessed it, image classification! For me, this lesson made the concepts of deep learning and fine-tuning pretrained models, which I have been doing for the duration of course, a lot clearer! Overall, a great course and very grateful to Jeremy for sharing his wisdom.
