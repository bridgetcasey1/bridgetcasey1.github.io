# Lesson 7
## Gradient Accumulation and GPU Memory
The first part of lesson 7 discussed the increased GPU memory required when working with larger models. Larger, more complex models involve more parameters, which allows for more precise adjustment and increased accuracy, however the trade-off is increased GPU memory usage with gradient calculations. Jeremy showed us a handy function for determining how much memory is used for training a model and to then clear out the memory:

```
import gc
def report_gpu():
    print(torch.cuda.list_gpu_processes())
    gc.collect()
    torch.cuda.empty_cache()
```

The above function can be used to compare GPU memory usage for different architectures. More information on GPU can be found [here](https://docs.fast.ai/dev/gpu.html).

The biggest takeaway from the lesson in my opinion was the concept of gradient accumulation. This allows the use of smaller batch sizes and thus less GPU memory to achieve the same performance as a larger batch size (called the effective batch size). It works by accumulating the gradients for multiple batches and then updating model weights accordingly rather than updating the model weights after every batch based on that batch's gradients[^1]. So many clever tricks in deep learning!

## Collaborative Filtering
The deep learning applications I am aware of continues to grow and grow with this lesson covering recommendation algorithms in the context of making movie recommendations based on previous user reviews.

![Movie Time](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/76d22afb-edaa-4079-941a-5c5f2e2d2969)

I have been impressed by recommendations made to me by algorithms in the past, so it was cool to learn how this works! I found the idea that you don't need to identify parameters that influence movie preferences but just indicate that some undefined parameters exist using latent factors and let fastai do the rest!

# Lesson 8
The last lesson! I have learnt a ridiculous amount during this course and feel it is only scraping the surface of deep learning and AI. I'm glad CNNs (convolutional neural networks) were discussed this lesson as it is a term I always that hear about in deep learning discussions but until this course really had no idea what they referred to. I always thought it was a very abstract concept however through the course have realised a neural network is just a mathematical function. 

A standard neural network is presented in this lesson which:
1. Multiplies each input by a number of values (parameters)
2. Sums these for each group of values
3. Replaces the negative numbers with zeros

This process is repeated to increase the number of layers of the neural network, with the input of one layer feeding into the next.

![239734129-26481608-68b6-4c2a-964e-8078b468fc22](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/f7dd3caf-f047-4bb5-9737-eaba089563e9)


So, what do we do with this? As the initial parameter values are chosen randomly, the neural network at first has no real function but these parameters can be adjusted using gradient descent to fit the data at hand. This allows application to so many different problems! For me, this lesson made the concepts of deep learning and fine-tuning pretrained models, which I have been doing for the duration of course, a lot clearer! Overall, a great course and very grateful to Jeremy for sharing his wisdom[^2].

![thank-you](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/f7b2a01b-71e8-497b-a56b-c949dbe70d59)

[^1]: [A useful gradient accumulation resource](https://www.run.ai/blog/gradient-accumulation)
[^2]: [Find the course here](https://course.fast.ai/)
