# Lesson 7
## Gradient Accumulation and GPU Memory
The first part of lesson 7 discussed the required GPU memory with larger models that require more and more parameters. Although increased accuracy is usually achieved, the gradients that have to be calculated chew up a lot of memory. Jeremy showed us a handy function function to find out how much memory is used for training a model and also to then clear out the memory:

```
import gc
def report_gpu():
    print(torch.cuda.list_gpu_processes())
    gc.collect()
    torch.cuda.empty_cache()
```

The above can be used to compare GPU memory usage for different architectures. The biggest takeaway from the lesson in my opinion was the concept of gradient accumulation. This allows us to use smaller batch sizes and thus less GPU memory but to get results of using a larger (effective) batch size. It works by accumulating the gradients for multiple batches and then updating model weights accordingly rather than updating the model weights after every batch based on that batch's gradients. So many clever tricks in deep learning!

## Multi-Target Models
On the topic of the genius of deep learning, Jeremy also demonstrated how easily the code we have been using can be adapted so that two predictions are made per image. The model has more to deal with now, it can actually improve the model compared to focusing on one category as the extra information may actually help the original prediction.

## Collaborative Filtering
Nearing the end of the course and the applications for deep learning keeps on growing and growing. This lesson also covers recommendation algorithms in the context of making movie recommendations based on previous user movie reviews.
![Movie Time](https://github.com/bridgetcasey1/bridgetcasey1.github.io/assets/113487655/76d22afb-edaa-4079-941a-5c5f2e2d2969)

I have been impressed by recommendations made to me by algorithms in the past so found it cool to learn about how these actually work! I found the idea that you don't need to identify parameters that influence movie preference but just use latent factors to say some exist and let the model do the rest. Any by the rest, I mean optimise the model in terms of loss function and determining parameter values.
